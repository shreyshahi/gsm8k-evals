# math-evals

We will benchmark several prompting techniques on mathematical datasets such as [gsm-8k](https://huggingface.co/datasets/openai/gsm8k) and [MATH]( https://github.com/hendrycks/math).

The ideas are all human generated, and code is mostly AI generated (cursor + claude-3.5-sonnet)

## Lab notebook / observations

## TODO
- [ ] Make the evaluation script general enough to handle multiple datasets
- [ ] Save traces for different datasets in different folders